{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'helpers' from '/Users/nikku/projects/vates/helpers.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "from helpers import one_hot, prepare_batches\n",
    "from random import seed, shuffle\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "\n",
    "from importlib import reload\n",
    "import helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/t8.shakespeare.txt') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare = []\n",
    "#start after the header\n",
    "skip = False\n",
    "for line in text.split(\"\\n\")[244:]:\n",
    "    if line[:2] == \"<<\":\n",
    "        skip = True\n",
    "    elif line[-2:] == \">>\":\n",
    "        skip = False\n",
    "    if skip or line == \"\":\n",
    "        continue\n",
    "    shakespeare.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113282"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(shakespeare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "927568"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getsizeof(shakespeare)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "flattened = flatten(shakespeare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = tuple(set(flattened))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {value: key for key, value in int2char.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#roughly 90% of sequences are shorter than 70 characters, so we'll make the max sequence length 70.\n",
    "sorted([len(l) for l in shakespeare], reverse=True)[1100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#90 % are longer than 39 characters... maybe we should have a shortest length too?\n",
    "[len(l) for l in shakespeare][1100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple character to number encoding\n",
    "#truncate sequences longer than 70\n",
    "numeric_sequences = [[char2int[char] for char in line][:70] for line in shakespeare]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(1609)\n",
    "#randomly shuffle the sequences\n",
    "shuffle(numeric_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split into Training and Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#90 - 10 training / validation split\n",
    "n_training_sequences = int(.9 * len(numeric_sequences))\n",
    "training = numeric_sequences[:n_training_sequences]\n",
    "validation = numeric_sequences[n_training_sequences:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11329, 101953)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation), len(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_input = [sequence[:-1] for sequence in training]\n",
    "training_target = [sequence[1:] for sequence in training]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'helpers' from '/Users/nikku/projects/vates/helpers.py'>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(helpers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "packed_batches = helpers.prepare_batches(training_input, batch_size = 20, n_states = len(chars), sequence_length = 69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "\n",
    "for packed_batch in packed_batches:\n",
    "    \n",
    "    unpacked_sequences, sequence_lengths = pad_packed_sequence(packed_batch)\n",
    "    \n",
    "    for i in range(len(sequence_lengths)):\n",
    "        \n",
    "        length = sequence_lengths[i]\n",
    "        sequence = unpacked_sequences[:,i,:][:length]\n",
    "        \n",
    "        numbers_sequence = [helpers.decode_one_hot(vec) for vec in sequence]\n",
    "        \n",
    "        lines.append([int2char[num] for num in numbers_sequence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    interim be but a se'nnight, Time's pace is so hard that it seems \n",
      "\n",
      "and three or four FOLLOWERS accordingly, with PORTIA, NERISSA, and tr\n",
      "\n",
      "Alarum. Excursions. Enter the King, the Prince, Lord John of Lancaste\n",
      "\n",
      "    John Doit of Staffordshire, and black George Barnes, and Francis \n",
      "\n",
      "  Ham. Let me see. [Takes the skull.] Alas, poor Yorick! I knew him, \n",
      "\n",
      "GLOUCESTER offers to put up a bill; WINCHESTER snatches it, and tears\n",
      "\n",
      "Enter King, Queen, Polonius, Ophelia, Rosencrantz, Guildenstern, and \n",
      "\n",
      "  SPEED. Marry, by these special marks: first, you have learn'd, like\n",
      "\n",
      "    never did such deeds in arms as I have done this day. I have paid\n",
      "\n",
      "Trumpets, sennet, and cornets. Enter two VERGERS, with short silver w\n",
      "\n",
      "  PANDARUS. Good boy, tell him I come.                       Exit Boy\n",
      "\n",
      "    it with security. I look'd 'a should have sent me two and twenty \n",
      "\n",
      "    thus much for greeting. Now, my spruce companions, is all ready, \n",
      "\n",
      "  LAUNCE. Out with that too; it was Eve's legacy, and cannot be ta'en\n",
      "\n",
      "    your son was upon his return home, I moved the King my master to \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(15):\n",
    "    print(''.join(lines[i]) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "del nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import _VF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.LSTM.forward_impl = monkey_patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monkey_patch(self, input, hx, batch_sizes, max_batch_size, sorted_indices):\n",
    "    # type: (Tensor, Optional[Tuple[Tensor, Tensor]], Optional[Tensor], int, Optional[Tensor]) -> Tuple[Tensor, Tuple[Tensor, Tensor]]  # noqa\n",
    "    if hx is None:\n",
    "        num_directions = 2 if self.bidirectional else 1\n",
    "        zeros = torch.zeros(self.num_layers * num_directions,\n",
    "                            max_batch_size, self.hidden_size,\n",
    "                            dtype=input.dtype, device=input.device)\n",
    "        hx = (zeros, zeros)\n",
    "    else:\n",
    "        # Each batch of the hidden state should match the input sequence that\n",
    "        # the user believes he/she is passing in.\n",
    "        hx = self.permute_hidden(hx, sorted_indices)\n",
    "\n",
    "    self.check_forward_args(input, hx, batch_sizes)\n",
    "    if batch_sizes is None:\n",
    "        result = _VF.lstm(input, hx, self._get_flat_weights(), self.bias, self.num_layers,\n",
    "                          self.dropout, self.training, self.bidirectional, self.batch_first)\n",
    "    else:\n",
    "        result = _VF.lstm(input, batch_sizes, hx, self._get_flat_weights(), bool(self.bias),\n",
    "                          self.num_layers, self.dropout, self.training, self.bidirectional)\n",
    "    output = result[0]\n",
    "    hidden = result[1:]\n",
    "\n",
    "    return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShakespeareNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_chars, hidden_size, n_rnn_layers=1, dropout=0):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_layers = n_rnn_layers\n",
    "        self.n_hidden = hidden_size\n",
    "        #input size corresponds to the number of unique characters\n",
    "        self.lstm = nn.LSTM(n_chars, hidden_size, n_rnn_layers, dropout)\n",
    "        \n",
    "        #decoder layer?\n",
    "        self.dense = nn.Linear(hidden_size, n_chars)\n",
    "        \n",
    "        \n",
    "    def forward(self, seq, hx):\n",
    "        \n",
    "        #ignore hidden \n",
    "        recurrent_output, _ = self.lstm(seq, hx)\n",
    "        \n",
    "        X, _ = pad_packed_sequence(recurrent_output)\n",
    "        \n",
    "        pdb.set_trace()\n",
    "        \n",
    "        out = self.dense(recurrent_output)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        \n",
    "        weight0 = next(self.parameters()).data\n",
    "        hidden = (weight0.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                  weight0.new(self.n_layers, batch_size, self.n_hidden).zero_())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "william = ShakespeareNN(83, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hx = william.init_hidden(20)\n",
    "william(packed_batch, hx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "lstm() received an invalid combination of arguments - got (Tensor, Tensor, tuple, list, int, int, float, bool, bool), but expected one of:\n * (Tensor data, Tensor batch_sizes, tuple of Tensors hx, tuple of Tensors params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional)\n      didn't match because some of the arguments have invalid types: (\u001b[32;1mTensor\u001b[0m, \u001b[32;1mTensor\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[31;1mlist\u001b[0m, \u001b[31;1mint\u001b[0m, \u001b[32;1mint\u001b[0m, \u001b[32;1mfloat\u001b[0m, \u001b[32;1mbool\u001b[0m, \u001b[32;1mbool\u001b[0m)\n * (Tensor input, tuple of Tensors hx, tuple of Tensors params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first)\n      didn't match because some of the arguments have invalid types: (\u001b[32;1mTensor\u001b[0m, \u001b[31;1mTensor\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[31;1mlist\u001b[0m, \u001b[32;1mint\u001b[0m, \u001b[32;1mint\u001b[0m, \u001b[31;1mfloat\u001b[0m, \u001b[32;1mbool\u001b[0m, \u001b[32;1mbool\u001b[0m)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-84c964e26907>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mhx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwilliam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mwilliam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-e58048185da1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, seq, hx)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mrecurrent_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecurrent_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_packed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward_packed\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0mmax_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_packed_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munsorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward_impl\u001b[0;34m(self, input, hx, batch_sizes, max_batch_size, sorted_indices)\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._get_flat_weights(), self.bias,\n\u001b[0;32m--> 525\u001b[0;31m                               self.num_layers, self.dropout, self.training, self.bidirectional)\n\u001b[0m\u001b[1;32m    526\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: lstm() received an invalid combination of arguments - got (Tensor, Tensor, tuple, list, int, int, float, bool, bool), but expected one of:\n * (Tensor data, Tensor batch_sizes, tuple of Tensors hx, tuple of Tensors params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional)\n      didn't match because some of the arguments have invalid types: (\u001b[32;1mTensor\u001b[0m, \u001b[32;1mTensor\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[31;1mlist\u001b[0m, \u001b[31;1mint\u001b[0m, \u001b[32;1mint\u001b[0m, \u001b[32;1mfloat\u001b[0m, \u001b[32;1mbool\u001b[0m, \u001b[32;1mbool\u001b[0m)\n * (Tensor input, tuple of Tensors hx, tuple of Tensors params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first)\n      didn't match because some of the arguments have invalid types: (\u001b[32;1mTensor\u001b[0m, \u001b[31;1mTensor\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[31;1mlist\u001b[0m, \u001b[32;1mint\u001b[0m, \u001b[32;1mint\u001b[0m, \u001b[31;1mfloat\u001b[0m, \u001b[32;1mbool\u001b[0m, \u001b[32;1mbool\u001b[0m)\n"
     ]
    }
   ],
   "source": [
    "packed_batch = packed_batches[0]\n",
    "with torch.no_grad():\n",
    "    hx = william.init_hidden(20)\n",
    "    william(packed_batch, hx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(input_size, hidden_size = 100, num_layers=2, dropout=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PackedSequence(data=tensor([[-0.0139,  0.0061, -0.0217,  ..., -0.0060, -0.0010, -0.0222],\n",
       "         [-0.0155, -0.0032, -0.0221,  ..., -0.0153, -0.0020, -0.0266],\n",
       "         [-0.0197,  0.0005, -0.0210,  ..., -0.0144,  0.0002, -0.0240],\n",
       "         ...,\n",
       "         [-0.0099, -0.0041, -0.0365,  ..., -0.0332, -0.0240, -0.0397],\n",
       "         [-0.0022, -0.0014, -0.0188,  ..., -0.0083, -0.0060, -0.0291],\n",
       "         [-0.0360,  0.0112, -0.0251,  ..., -0.0252, -0.0176, -0.0272]],\n",
       "        grad_fn=<CatBackward>), batch_sizes=tensor([20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "         20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "         20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "         20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20]), sorted_indices=None, unsorted_indices=None),\n",
       " (tensor([[[-0.0758, -0.0262,  0.0271,  ...,  0.0043, -0.0169, -0.0479],\n",
       "           [-0.0966, -0.0695,  0.0326,  ..., -0.0045,  0.0137, -0.0116],\n",
       "           [-0.1049, -0.0629,  0.0411,  ...,  0.0051, -0.0073, -0.0049],\n",
       "           ...,\n",
       "           [-0.0903, -0.0479,  0.0301,  ...,  0.0060, -0.0049, -0.0230],\n",
       "           [-0.0893, -0.0338,  0.0441,  ...,  0.0084, -0.0357, -0.0323],\n",
       "           [-0.0684, -0.0343,  0.0501,  ..., -0.0122, -0.0295, -0.0417]],\n",
       "  \n",
       "          [[-0.0286,  0.0029, -0.0235,  ..., -0.0170, -0.0022, -0.0264],\n",
       "           [-0.0150,  0.0067, -0.0381,  ..., -0.0205, -0.0032, -0.0431],\n",
       "           [-0.0200,  0.0036, -0.0437,  ..., -0.0196, -0.0137, -0.0350],\n",
       "           ...,\n",
       "           [-0.0099, -0.0041, -0.0365,  ..., -0.0332, -0.0240, -0.0397],\n",
       "           [-0.0022, -0.0014, -0.0188,  ..., -0.0083, -0.0060, -0.0291],\n",
       "           [-0.0360,  0.0112, -0.0251,  ..., -0.0252, -0.0176, -0.0272]]],\n",
       "         grad_fn=<StackBackward>),\n",
       "  tensor([[[-0.1741, -0.0529,  0.0534,  ...,  0.0082, -0.0316, -0.1022],\n",
       "           [-0.2198, -0.1352,  0.0661,  ..., -0.0090,  0.0245, -0.0251],\n",
       "           [-0.2217, -0.1172,  0.0811,  ...,  0.0102, -0.0143, -0.0109],\n",
       "           ...,\n",
       "           [-0.1996, -0.0919,  0.0586,  ...,  0.0119, -0.0092, -0.0485],\n",
       "           [-0.2059, -0.0684,  0.0873,  ...,  0.0159, -0.0666, -0.0687],\n",
       "           [-0.1585, -0.0695,  0.0990,  ..., -0.0229, -0.0550, -0.0880]],\n",
       "  \n",
       "          [[-0.0575,  0.0055, -0.0429,  ..., -0.0328, -0.0042, -0.0512],\n",
       "           [-0.0307,  0.0126, -0.0685,  ..., -0.0389, -0.0060, -0.0827],\n",
       "           [-0.0415,  0.0066, -0.0790,  ..., -0.0375, -0.0263, -0.0685],\n",
       "           ...,\n",
       "           [-0.0200, -0.0075, -0.0663,  ..., -0.0648, -0.0464, -0.0781],\n",
       "           [-0.0044, -0.0027, -0.0346,  ..., -0.0158, -0.0113, -0.0566],\n",
       "           [-0.0732,  0.0210, -0.0456,  ..., -0.0482, -0.0333, -0.0533]]],\n",
       "         grad_fn=<StackBackward>)))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm(packed_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.NLLLoss()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
